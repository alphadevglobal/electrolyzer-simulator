# Projeto IA AV3
## ClassificaÃ§Ã£o de Dados de Energia de EletrodomÃ©sticos

**Disciplina:** InteligÃªncia Artificial Computacional
**Professor:** Ms. Cynthia Moreira Maia
**Aluno:** Mateus Gomes MacÃ¡rio
**Data:** 26/11/2025

---

## Agenda

1. IntroduÃ§Ã£o
2. Dataset e Justificativa
3. Algoritmos Implementados
4. MÃ©tricas de AvaliaÃ§Ã£o
5. Experimentos e ValidaÃ§Ã£o Cruzada
6. Resultados
7. AnÃ¡lise Comparativa
8. ConclusÃµes
9. ReferÃªncias

---

## 1. IntroduÃ§Ã£o

### Objetivo do Projeto
- Avaliar diferentes algoritmos de classificaÃ§Ã£o
- ImplementaÃ§Ã£o **100% manual** (sem pandas, sem scikit-learn)
- Comparar desempenho usando validaÃ§Ã£o cruzada

### Problema
ClassificaÃ§Ã£o binÃ¡ria do consumo de energia de eletrodomÃ©sticos

---

## 2. Dataset: Appliances Energy Prediction

### CaracterÃ­sticas
- **Fonte:** OpenML (ID: 46283)
- **InstÃ¢ncias:** 5,000 (dataset de demonstraÃ§Ã£o)
- **Features:** 28 atributos
- **Target:** Consumo de energia (binarizado)

### Justificativa da Escolha
âœ… Mais de 10 atributos (requisito: >10)
âœ… Mais de 1000 instÃ¢ncias (requisito: >1000)
âœ… Alinhamento com pesquisa em eficiÃªncia energÃ©tica
âœ… Dados reais de consumo residencial

---

## 2.1 DistribuiÃ§Ã£o dos Dados

### Classes Balanceadas
- **Classe 0:** 2,500 amostras (50%)
- **Classe 1:** 2,500 amostras (50%)

### PrÃ©-processamento
- NormalizaÃ§Ã£o Z-Score (implementada manualmente)
- BinarizaÃ§Ã£o do target pela mediana
- DivisÃ£o estratificada em K-Folds

---

## 3. Algoritmos Implementados

### Todos implementados do ZERO!
**Nenhuma biblioteca de ML foi utilizada**

1. **K-Nearest Neighbors (KNN)**
2. **Perceptron**
3. **Multi-Layer Perceptron (MLP)**
4. **Naive Bayes**

---

## 3.1 K-Nearest Neighbors (KNN)

### ImplementaÃ§Ã£o
```python
class KNearestNeighbors:
    def euclidean_distance(self, x1, x2):
        return np.sqrt(np.sum((x1 - x2) ** 2))

    def manhattan_distance(self, x1, x2):
        return np.sum(np.abs(x1 - x2))
```

### VariaÃ§Ãµes Testadas
- **DistÃ¢ncia Euclidiana** (k=5)
- **DistÃ¢ncia Manhattan** (k=5)

### CaracterÃ­sticas
- Algoritmo lazy (sem fase de treino)
- Busca exaustiva dos k vizinhos
- VotaÃ§Ã£o majoritÃ¡ria

---

## 3.2 Perceptron

### ImplementaÃ§Ã£o
```python
class MultiClassPerceptron:
    def fit(self, X, y):
        for epoch in range(self.n_epochs):
            for x_i, y_i in zip(X, y):
                y_pred = self.predict_single(x_i)
                error = y_i - y_pred
                if error != 0:
                    self.weights += self.lr * error * x_i
                    self.bias += self.lr * error
```

### ParÃ¢metros
- Taxa de aprendizado: 0.01
- Ã‰pocas: 50
- EstratÃ©gia: One-vs-Rest para multiclasse

---

## 3.3 Multi-Layer Perceptron (MLP)

### Arquitetura
- **Input Layer:** 28 neurÃ´nios
- **Hidden Layer 1:** 32 neurÃ´nios (ReLU)
- **Hidden Layer 2:** 16 neurÃ´nios (ReLU)
- **Output Layer:** 2 neurÃ´nios (Softmax)

### Treinamento
- **Backpropagation** implementado manualmente
- Mini-batch Gradient Descent (batch=64)
- Taxa de aprendizado: 0.01
- Ã‰pocas: 50

---

## 3.3.1 Backpropagation Manual

```python
def backward_propagation(self, X, y, activations, z_values):
    # Erro na camada de saÃ­da
    delta = activations[-1] - y

    # Propaga erro para camadas anteriores
    for i in reversed(range(n_layers)):
        gradients_w[i] = np.dot(activations[i].T, delta) / m
        gradients_b[i] = np.sum(delta, axis=0) / m

        if i > 0:
            delta = np.dot(delta, self.weights[i].T) * \
                    self.activation_derivative(z_values[i-1])

    return gradients_w, gradients_b
```

---

## 3.4 Naive Bayes

### ImplementaÃ§Ãµes
1. **VersÃ£o Univariada**
   - Assume independÃªncia entre features
   - Mais rÃ¡pido e simples

2. **VersÃ£o Multivariada**
   - Considera correlaÃ§Ã£o entre features
   - Usa distribuiÃ§Ã£o Gaussiana multivariada

### PDF Gaussiana Manual
```python
def gaussian_pdf(self, x, mean, std):
    exponent = np.exp(-((x - mean) ** 2) / (2 * std ** 2))
    return (1 / (np.sqrt(2 * np.pi) * std)) * exponent
```

---

## 4. MÃ©tricas de AvaliaÃ§Ã£o

### Implementadas Manualmente

#### 1. AcurÃ¡cia (Accuracy)
```python
def accuracy_score(y_true, y_pred):
    correct = sum(1 for i in range(len(y_true))
                  if y_true[i] == y_pred[i])
    return correct / len(y_true)
```

#### 2. PrecisÃ£o (Precision)
- TP / (TP + FP)

#### 3. F1-Score
- 2 Ã— (Precision Ã— Recall) / (Precision + Recall)

---

## 5. ValidaÃ§Ã£o Cruzada

### K-Fold Cross-Validation (k=5)
**ImplementaÃ§Ã£o 100% manual!**

```python
def k_fold_split(X, y, n_folds=5):
    indices = np.random.permutation(len(X))
    fold_size = len(X) // n_folds

    for i in range(n_folds):
        val_indices = indices[i*fold_size:(i+1)*fold_size]
        train_indices = np.concatenate([
            indices[:i*fold_size],
            indices[(i+1)*fold_size:]
        ])
        yield train_indices, val_indices
```

### EstratificaÃ§Ã£o
- MantÃ©m proporÃ§Ã£o das classes em cada fold
- Reduz variÃ¢ncia das estimativas

---

## 6. Resultados Completos

| Classificador | AcurÃ¡cia | PrecisÃ£o | F1-Score | Tempo Treino (s) | Tempo Teste (s) |
|--------------|----------|----------|----------|------------------|------------------|
| KNN (Euclidiana) | 0.80 Â± 0.01 | 0.80 Â± 0.01 | 0.80 Â± 0.01 | 0.00 Â± 0.00 | 13.09 Â± 0.61 |
| KNN (Manhattan) | 0.81 Â± 0.01 | 0.81 Â± 0.01 | 0.81 Â± 0.01 | 0.00 Â± 0.00 | 10.19 Â± 0.11 |
| Perceptron | 0.77 Â± 0.02 | 0.84 Â± 0.01 | 0.80 Â± 0.01 | 0.36 Â± 0.04 | 0.00 Â± 0.00 |
| **MLP** | **0.93 Â± 0.01** | **0.93 Â± 0.01** | **0.93 Â± 0.01** | 0.22 Â± 0.01 | 0.00 Â± 0.00 |
| Naive Bayes (Univariado) | 0.91 Â± 0.01 | 0.91 Â± 0.01 | 0.91 Â± 0.01 | 0.00 Â± 0.00 | 0.12 Â± 0.00 |
| Naive Bayes (Multivariado) | 0.92 Â± 0.01 | 0.92 Â± 0.01 | 0.92 Â± 0.01 | 0.00 Â± 0.00 | 0.05 Â± 0.00 |

---

## 7. AnÃ¡lise Comparativa

### Por Desempenho (AcurÃ¡cia)
1. **MLP:** 92.52% â­
2. **Naive Bayes (Multivariado):** 91.62%
3. **Naive Bayes (Univariado):** 90.90%
4. **KNN (Manhattan):** 80.88%
5. **KNN (Euclidiana):** 80.04%
6. **Perceptron:** 77.08%

### Por Tempo de Treinamento
1. **KNN (ambos):** ~0.00s (lazy learning)
2. **Naive Bayes (ambos):** ~0.00s
3. **MLP:** 0.22s
4. **Perceptron:** 0.36s

---

## 7.1 Trade-off: Desempenho vs EficiÃªncia

### Score de EficiÃªncia = AcurÃ¡cia / Tempo Total

| Classificador | Score |
|--------------|-------|
| **Naive Bayes (Multivariado)** | **14.08** ðŸ† |
| Naive Bayes (Univariado) | 7.07 |
| MLP | 3.97 |
| Perceptron | 2.05 |
| KNN (Manhattan) | 0.08 |
| KNN (Euclidiana) | 0.06 |

---

## 7.2 ComparaÃ§Ã£o das MÃ©tricas

### Observations

**Melhores Classificadores:**
- **AcurÃ¡cia:** MLP (92.52%)
- **F1-Score:** MLP (92.53%)
- **EficiÃªncia:** Naive Bayes Multivariado

**DiferenÃ§as entre KNN:**
- Manhattan levemente superior Ã  Euclidiana
- Manhattan mais rÃ¡pido no teste

**Perceptron:**
- Menor acurÃ¡cia, mas tempo razoÃ¡vel
- Boa precisÃ£o (84%) mas recall inferior

---

## 7.3 AnÃ¡lise dos Tempos

### Tempo de Treino
- **Redes Neurais (MLP):** 0.22s
  - Backpropagation computacionalmente intensivo
- **Perceptron:** 0.36s
  - MÃºltiplas Ã©pocas com atualizaÃ§Ã£o de pesos
- **KNN:** ~0.00s
  - Lazy learning: apenas armazena dados

### Tempo de Teste
- **KNN:** 10-13s âš ï¸
  - Busca exaustiva em todo dataset
  - Cresce linearmente com tamanho do dataset
- **Naive Bayes:** 0.05-0.12s
  - Apenas cÃ¡lculo de probabilidades
- **Redes Neurais:** ~0.00s
  - Forward pass muito rÃ¡pido

---

## 7.4 Pontos Fortes e LimitaÃ§Ãµes

### KNN
âœ… Sem fase de treinamento
âœ… Simples de implementar
âŒ Muito lento no teste
âŒ SensÃ­vel a features irrelevantes

### Perceptron
âœ… Simples e interpretÃ¡vel
âœ… RÃ¡pido no teste
âŒ Apenas linear
âŒ Menor acurÃ¡cia

---

## 7.4 Pontos Fortes e LimitaÃ§Ãµes (cont.)

### MLP
âœ… **Melhor acurÃ¡cia** (92.52%)
âœ… Captura relaÃ§Ãµes nÃ£o-lineares
âœ… RÃ¡pido no teste
âŒ DifÃ­cil de interpretar
âŒ Requer tuning de hiperparÃ¢metros

### Naive Bayes
âœ… **Extremamente rÃ¡pido**
âœ… Robusto com poucos dados
âœ… Alta acurÃ¡cia (>90%)
âŒ Assume independÃªncia das features

---

## 8. ConclusÃµes

### Melhor Classificador: **MLP**
- Maior acurÃ¡cia: **92.52% Â± 1.46%**
- Tempo de treino aceitÃ¡vel: **0.22s**
- Tempo de teste excelente: **~0.00s**

### Melhor Trade-off: **Naive Bayes Multivariado**
- AcurÃ¡cia competitiva: **91.62%**
- Tempo total imbatÃ­vel: **0.06s**
- **Score de eficiÃªncia: 14.08**

---

## 8.1 Insights Obtidos

1. **Deep Learning funciona!**
   - MLP superou todos os modelos clÃ¡ssicos

2. **Simplicidade Ã s vezes Ã© melhor**
   - Naive Bayes: simples, rÃ¡pido e eficaz

3. **KNN tem limitaÃ§Ãµes**
   - Excelente para datasets pequenos
   - ImpraticÃ¡vel para produÃ§Ã£o em larga escala

4. **Perceptron Ã© limitado**
   - Problema nÃ£o Ã© linearmente separÃ¡vel
   - MLP essencial para melhor desempenho

---

## 8.2 AplicaÃ§Ãµes PrÃ¡ticas

### Para ProduÃ§Ã£o: **Naive Bayes Multivariado**
- BaixÃ­ssima latÃªncia
- Alta acurÃ¡cia
- Ideal para sistemas em tempo real

### Para MÃ¡xima PrecisÃ£o: **MLP**
- Quando acurÃ¡cia Ã© crÃ­tica
- Recursos computacionais disponÃ­veis
- AplicaÃ§Ãµes cientÃ­ficas

---

## 9. ImplementaÃ§Ã£o TÃ©cnica

### Requisitos Atendidos âœ…
- âœ… Dataset com >10 atributos (28)
- âœ… Dataset com >1000 instÃ¢ncias (5000)
- âœ… 4 algoritmos implementados manualmente
- âœ… **SEM pandas**
- âœ… **SEM scikit-learn**
- âœ… ValidaÃ§Ã£o cruzada manual
- âœ… MÃ©tricas implementadas manualmente

### Tecnologias
- Python 3.x
- NumPy (apenas para arrays)
- Matplotlib (apenas para visualizaÃ§Ã£o)

---

## 10. CÃ³digo DisponÃ­vel

### RepositÃ³rio GitHub
**github.com/mateusmacario/electrolyzer-simulator**

### Estrutura
```
projeto_ia_av3/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ algorithms/      # KNN, Perceptron, MLP, Naive Bayes
â”‚   â”œâ”€â”€ utils/          # MÃ©tricas, CV, preprocessing
â”‚   â””â”€â”€ main.py         # Script principal
â”œâ”€â”€ data/               # Dataset
â”œâ”€â”€ results/            # Tabelas e grÃ¡ficos
â””â”€â”€ README.md          # DocumentaÃ§Ã£o
```

---

## 11. ReferÃªncias

1. **Dataset:**
   - Appliances Energy Prediction (OpenML ID: 46283)
   - https://www.openml.org/d/46283

2. **Algoritmos:**
   - Bishop, C.M. (2006). Pattern Recognition and Machine Learning
   - Hastie, T. et al. (2009). The Elements of Statistical Learning

3. **ImplementaÃ§Ã£o:**
   - CÃ³digo 100% original
   - Baseado em fundamentos teÃ³ricos

---

## Obrigado!

### Mateus Gomes MacÃ¡rio
**Engenharia da ComputaÃ§Ã£o - UNIFOR**

**Disciplina:** InteligÃªncia Artificial Computacional
**Professor:** Ms. Cynthia Moreira Maia

### Contato
- GitHub: github.com/mateusmacario
- Email: [seu-email]

---

## Perguntas?

ðŸ’¬ Estou Ã  disposiÃ§Ã£o para esclarecer dÃºvidas!
